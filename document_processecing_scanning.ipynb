{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/likeshd/ocr_work/blob/main/document_processecing_scanning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhnpGunDomy5",
        "outputId": "08a859fd-53d6-4f6e-b520-b00fca4d41d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract==0.3.10\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract==0.3.10) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract==0.3.10) (11.1.0)\n",
            "Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract==0.3.10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install libtesseract-dev tesseract-ocr-eng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4YxuBslrn9a",
        "outputId": "fa3d1dda-1c86-498e-8654-7b4e2b76a0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev tesseract-ocr tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev tesseract-ocr\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 6 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 8,560 kB of archives.\n",
            "After this operation, 31.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.3 [581 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 8,560 kB in 0s (17.6 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 124565 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libarchive-dev_3.6.0-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.3) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../1-libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../2-libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "Preparing to unpack .../3-tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../4-tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../5-tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.3) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "alauF0PHpXVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pytesseract\n",
        "from scipy.spatial import distance as dist\n",
        "import imutils\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.filters import threshold_local\n",
        "import time"
      ],
      "metadata": {
        "id": "YfR78NOwoobn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentScanner:\n",
        "    def __init__(self):\n",
        "        # Initialize OCR engine\n",
        "        self.tesseract_config = r'--oem 3 --psm 6'\n",
        "\n",
        "        # Initialize image enhancement parameters\n",
        "        self.kernel_size = (5, 5)\n",
        "        self.sigma = 1.0\n",
        "\n",
        "        # Performance tracking\n",
        "        self.processing_times = []\n",
        "        self.start_time = None\n",
        "\n",
        "    def preprocess_image(self, image):\n",
        "        \"\"\"Prepare image for document detection\"\"\"\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Apply Gaussian blur to reduce noise\n",
        "        blurred = cv2.GaussianBlur(gray, self.kernel_size, self.sigma)\n",
        "\n",
        "        # Apply edge detection\n",
        "        edges = cv2.Canny(blurred, 75, 200)\n",
        "\n",
        "        return edges\n",
        "\n",
        "    def find_document_contour(self, edges):\n",
        "        \"\"\"Detect the document boundaries\"\"\"\n",
        "        # Find contours in the edge map\n",
        "        contours = cv2.findContours(\n",
        "            edges.copy(),\n",
        "            cv2.RETR_LIST,\n",
        "            cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "        contours = imutils.grab_contours(contours)\n",
        "\n",
        "        # Sort contours by area in descending order\n",
        "        contours = sorted(\n",
        "            contours,\n",
        "            key=cv2.contourArea,\n",
        "            reverse=True\n",
        "        )[:5]\n",
        "\n",
        "        # Initialize document contour\n",
        "        document_contour = None\n",
        "\n",
        "        # Loop over contours\n",
        "        for contour in contours:\n",
        "            # Approximate the contour\n",
        "            perimeter = cv2.arcLength(contour, True)\n",
        "            approx = cv2.approxPolyDP(\n",
        "                contour,\n",
        "                0.02 * perimeter,\n",
        "                True\n",
        "            )\n",
        "\n",
        "            # If we have found a contour with four points,\n",
        "            # we can assume we have found the document\n",
        "            if len(approx) == 4:\n",
        "                document_contour = approx\n",
        "                break\n",
        "\n",
        "        return document_contour\n",
        "\n",
        "    def order_points(self, points):\n",
        "        \"\"\"Order points in top-left, top-right, bottom-right, bottom-left order\"\"\"\n",
        "        # Initialize coordinates\n",
        "        rect = np.zeros((4, 2), dtype=\"float32\")\n",
        "\n",
        "        # Get points sum and difference\n",
        "        pts_sum = points.sum(axis=1)\n",
        "        pts_diff = np.diff(points, axis=1)\n",
        "\n",
        "        # Top-left point has smallest sum\n",
        "        rect[0] = points[np.argmin(pts_sum)]\n",
        "        # Bottom-right point has largest sum\n",
        "        rect[2] = points[np.argmax(pts_sum)]\n",
        "        # Top-right point has smallest difference\n",
        "        rect[1] = points[np.argmin(pts_diff)]\n",
        "        # Bottom-left point has largest difference\n",
        "        rect[3] = points[np.argmax(pts_diff)]\n",
        "\n",
        "        return rect\n",
        "\n",
        "    def perspective_transform(self, image, points):\n",
        "        \"\"\"Apply perspective transform to obtain top-down view\"\"\"\n",
        "        # Order points in standard order\n",
        "        rect = self.order_points(\n",
        "            points.reshape(4, 2).astype(\"float32\")\n",
        "        )\n",
        "        (tl, tr, br, bl) = rect\n",
        "\n",
        "        # Calculate width of new image\n",
        "        widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
        "        widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
        "        max_width = max(int(widthA), int(widthB))\n",
        "\n",
        "        # Calculate height of new image\n",
        "        heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
        "        heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
        "        max_height = max(int(heightA), int(heightB))\n",
        "\n",
        "        # Construct destination points\n",
        "        dst = np.array([\n",
        "            [0, 0],\n",
        "            [max_width - 1, 0],\n",
        "            [max_width - 1, max_height - 1],\n",
        "            [0, max_height - 1]\n",
        "        ], dtype=\"float32\")\n",
        "\n",
        "        # Calculate perspective transform matrix\n",
        "        transform_matrix = cv2.getPerspectiveTransform(rect, dst)\n",
        "\n",
        "        # Apply perspective transform\n",
        "        warped = cv2.warpPerspective(\n",
        "            image,\n",
        "            transform_matrix,\n",
        "            (max_width, max_height)\n",
        "        )\n",
        "\n",
        "        return warped\n",
        "\n",
        "    def enhance_document(self, image):\n",
        "        \"\"\"Enhance document image for better readability\"\"\"\n",
        "        # Convert to grayscale if needed\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = image\n",
        "\n",
        "        # Apply adaptive thresholding\n",
        "        threshold = threshold_local(gray, 11, offset=10)\n",
        "        binary = (gray > threshold).astype(\"uint8\") * 255\n",
        "\n",
        "        # Apply unsharp masking for edge enhancement\n",
        "        blurred = cv2.GaussianBlur(binary, (0, 0), 3)\n",
        "        enhanced = cv2.addWeighted(binary, 1.5, blurred, -0.5, 0)\n",
        "\n",
        "        return enhanced\n",
        "\n",
        "    def extract_text(self, image):\n",
        "        \"\"\"Extract text from the document image\"\"\"\n",
        "        # Ensure image is in correct format\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "        # Extract text using Tesseract\n",
        "        text = pytesseract.image_to_string(\n",
        "            image,\n",
        "            config=self.tesseract_config\n",
        "        )\n",
        "\n",
        "        return text\n",
        "\n",
        "    def detect_text_regions(self, image):\n",
        "        \"\"\"Detect and highlight text regions in the document\"\"\"\n",
        "        # Get image dimensions\n",
        "        height, width = image.shape[:2]\n",
        "\n",
        "        # Configure Tesseract to output bounding boxes\n",
        "        boxes = pytesseract.image_to_boxes(\n",
        "            image,\n",
        "            config=self.tesseract_config\n",
        "        )\n",
        "\n",
        "        # Create copy for visualization\n",
        "        visualization = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "        # Draw boxes around text\n",
        "        for box in boxes.splitlines():\n",
        "            box = box.split()\n",
        "            x, y, w, h = int(box[1]), int(box[2]), int(box[3]), int(box[4])\n",
        "\n",
        "            # Convert coordinates (OpenCV uses top-left origin)\n",
        "            y = height - y\n",
        "            h = height - h\n",
        "\n",
        "            # Draw rectangle around character\n",
        "            cv2.rectangle(\n",
        "                visualization,\n",
        "                (x, h),\n",
        "                (w, y),\n",
        "                (0, 255, 0),\n",
        "                1\n",
        "            )\n",
        "\n",
        "        return visualization\n",
        "\n",
        "    def process_document(self, image_path):\n",
        "        \"\"\"Process document image end-to-end\"\"\"\n",
        "        # Start timing\n",
        "        self.start_time = time.time()\n",
        "\n",
        "        # Read image\n",
        "        image = cv2.imread(image_path)\n",
        "        original = image.copy()\n",
        "\n",
        "        # Preprocess image\n",
        "        edges = self.preprocess_image(image)\n",
        "\n",
        "        # Find document contour\n",
        "        document_contour = self.find_document_contour(edges)\n",
        "\n",
        "        if document_contour is None:\n",
        "            raise ValueError(\"No document found in image\")\n",
        "\n",
        "        # Apply perspective transform\n",
        "        warped = self.perspective_transform(\n",
        "            original,\n",
        "            document_contour\n",
        "        )\n",
        "\n",
        "        # Enhance document\n",
        "        enhanced = self.enhance_document(warped)\n",
        "\n",
        "        # Extract text\n",
        "        text = self.extract_text(enhanced)\n",
        "\n",
        "        # Detect text regions\n",
        "        text_visualization = self.detect_text_regions(enhanced)\n",
        "\n",
        "        # Track processing time\n",
        "        processing_time = time.time() - self.start_time\n",
        "        self.processing_times.append(processing_time)\n",
        "\n",
        "        return {\n",
        "            'original': original,\n",
        "            'edges': edges,\n",
        "            'warped': warped,\n",
        "            'enhanced': enhanced,\n",
        "            'text_regions': text_visualization,\n",
        "            'text': text,\n",
        "            'processing_time': processing_time\n",
        "        }\n",
        "\n",
        "    def analyze_document_structure(self, image):\n",
        "        \"\"\"Analyze document structure and layout\"\"\"\n",
        "        # Get document structure using Tesseract\n",
        "        data = pytesseract.image_to_data(\n",
        "            image,\n",
        "            config=self.tesseract_config,\n",
        "            output_type=pytesseract.Output.DICT\n",
        "        )\n",
        "\n",
        "        # Initialize structure analysis\n",
        "        structure = {\n",
        "            'paragraphs': [],\n",
        "            'lines': [],\n",
        "            'words': []\n",
        "        }\n",
        "\n",
        "        # Process structure data\n",
        "        n_boxes = len(data['text'])\n",
        "        for i in range(n_boxes):\n",
        "            if int(data['conf'][i]) > 60:  # Filter by confidence\n",
        "                (x, y, w, h) = (\n",
        "                    data['left'][i],\n",
        "                    data['top'][i],\n",
        "                    data['width'][i],\n",
        "                    data['height'][i]\n",
        "                )\n",
        "\n",
        "                # Categorize by block type\n",
        "                if data['level'][i] == 4:  # Paragraph\n",
        "                    structure['paragraphs'].append({\n",
        "                        'text': data['text'][i],\n",
        "                        'bbox': (x, y, w, h)\n",
        "                    })\n",
        "                elif data['level'][i] == 5:  # Line\n",
        "                    structure['lines'].append({\n",
        "                        'text': data['text'][i],\n",
        "                        'bbox': (x, y, w, h)\n",
        "                    })\n",
        "                elif data['level'][i] == 6:  # Word\n",
        "                    structure['words'].append({\n",
        "                        'text': data['text'][i],\n",
        "                        'bbox': (x, y, w, h)\n",
        "                    })\n",
        "\n",
        "        return structure"
      ],
      "metadata": {
        "id": "XHwo4-Chooem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Create scanner instance\n",
        "    scanner = DocumentScanner()\n",
        "\n",
        "    # Process document\n",
        "    results = scanner.process_document('/content/DL B.jpeg')\n",
        "\n",
        "    # # Display results\n",
        "    # plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # plt.subplot(231)\n",
        "    # plt.imshow(cv2.cvtColor(results['original'], cv2.COLOR_BGR2RGB))\n",
        "    # plt.title('Original Image')\n",
        "\n",
        "    # plt.subplot(232)\n",
        "    # plt.imshow(results['edges'], cmap='gray')\n",
        "    # plt.title('Edge Detection')\n",
        "\n",
        "    # plt.subplot(233)\n",
        "    # plt.imshow(cv2.cvtColor(results['warped'], cv2.COLOR_BGR2RGB))\n",
        "    # plt.title('Perspective Transform')\n",
        "\n",
        "    # plt.subplot(234)\n",
        "    # plt.imshow(results['enhanced'], cmap='gray')\n",
        "    # plt.title('Enhanced Document')\n",
        "\n",
        "    # plt.subplot(235)\n",
        "    # plt.imshow(cv2.cvtColor(\n",
        "    #     results['text_regions'],\n",
        "    #     cv2.COLOR_BGR2RGB\n",
        "    # ))\n",
        "    # plt.title('Text Regions')\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "    print(f\"Extracted Text:\\n{results['text']}\")\n",
        "    print(f\"Processing Time: {results['processing_time']:.2f} seconds\")"
      ],
      "metadata": {
        "id": "LFDy9e4WoohX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt6-xN9oqLm4",
        "outputId": "d1b6f8c7-f8f9-4af6-d5bc-5084788f88a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text:\n",
            "\f\n",
            "Processing Time: 0.28 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eHDBdZiNqLpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bRNS1hikookG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "46nLtuUTs4Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_document(self, image, text):\n",
        "    \"\"\"Classify document type based on content and layout\"\"\"\n",
        "    def extract_features(self, image, text):\n",
        "        \"\"\"Extract features for classification\"\"\"\n",
        "        # Layout features\n",
        "        layout = self.analyze_document_structure(image)\n",
        "        layout_features = {\n",
        "            'n_paragraphs': len(layout['paragraphs']),\n",
        "            'n_lines': len(layout['lines']),\n",
        "            'text_density': len(text) / (image.shape[0] * image.shape[1])\n",
        "        }\n",
        "\n",
        "        # Content features\n",
        "        content_features = {\n",
        "            'has_date': bool(re.search(r'\\d{1,2}/\\d{1,2}/\\d{4}', text)),\n",
        "            'has_currency': bool(re.search(r'\\$\\d+\\.?\\d*', text)),\n",
        "            'has_letterhead': self.detect_letterhead(image)\n",
        "        }\n",
        "\n",
        "        return {**layout_features, **content_features}\n",
        "\n",
        "    def classify(self, features):\n",
        "        \"\"\"Classify document based on features\"\"\"\n",
        "        # Implement classification logic\n",
        "        if features['has_letterhead'] and features['text_density'] < 0.2:\n",
        "            return 'Letter'\n",
        "        elif features['has_currency'] and features['text_density'] > 0.3:\n",
        "            return 'Invoice'\n",
        "        elif features['has_date'] and features['n_paragraphs'] > 5:\n",
        "            return 'Report'\n",
        "        else:\n",
        "            return 'General Document'\n",
        "\n",
        "    # Extract features and classify\n",
        "    features = extract_features(self, image, text)\n",
        "    return classify(self, features)\n",
        "\n",
        "def detect_letterhead(self, image):\n",
        "    \"\"\"Detect presence of letterhead in document\"\"\"\n",
        "    # Analyze top portion of document\n",
        "    top_section = image[:int(image.shape[0] * 0.2), :]\n",
        "\n",
        "    # Apply text detection to top section\n",
        "    top_text = pytesseract.image_to_data(\n",
        "        top_section,\n",
        "        config=self.tesseract_config,\n",
        "        output_type=pytesseract.Output.DICT\n",
        "    )\n",
        "\n",
        "    # Check for company indicators\n",
        "    has_logo = self.detect_logo(top_section)\n",
        "    has_company_name = any(\n",
        "        len(word) > 3 and word.isupper()\n",
        "        for word in top_text['text']\n",
        "        if isinstance(word, str)\n",
        "    )\n",
        "\n",
        "    return has_logo or has_company_name\n",
        "\n",
        "def detect_logo(self, image):\n",
        "    \"\"\"Detect presence of logo in image section\"\"\"\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply adaptive thresholding\n",
        "    thresh = cv2.adaptiveThreshold(\n",
        "        gray, 255,\n",
        "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "        cv2.THRESH_BINARY_INV, 11, 2\n",
        "    )\n",
        "\n",
        "    # Find contours\n",
        "    contours = cv2.findContours(\n",
        "        thresh,\n",
        "        cv2.RETR_EXTERNAL,\n",
        "        cv2.CHAIN_APPROX_SIMPLE\n",
        "    )[0]\n",
        "\n",
        "    # Filter contours by size and shape\n",
        "    for contour in contours:\n",
        "        area = cv2.contourArea(contour)\n",
        "        if 100 < area < 5000:  # Typical logo size range\n",
        "            x, y, w, h = cv2.boundingRect(contour)\n",
        "            aspect_ratio = w / float(h)\n",
        "            if 0.5 < aspect_ratio < 2:  # Typical logo aspect ratio\n",
        "                return True\n",
        "\n",
        "    return False\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Advanced document processing capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, scanner):\n",
        "        self.scanner = scanner\n",
        "        self.document_history = []\n",
        "\n",
        "    def batch_process(self, image_paths):\n",
        "        \"\"\"Process multiple documents in batch\"\"\"\n",
        "        results = []\n",
        "        for path in image_paths:\n",
        "            try:\n",
        "                result = self.scanner.process_document(path)\n",
        "                doc_type = self.scanner.classify_document(\n",
        "                    result['enhanced'],\n",
        "                    result['text']\n",
        "                )\n",
        "                result['document_type'] = doc_type\n",
        "                results.append(result)\n",
        "                self.document_history.append({\n",
        "                    'path': path,\n",
        "                    'type': doc_type,\n",
        "                    'timestamp': time.time()\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {path}: {str(e)}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_searchable_pdf(self, image, text):\n",
        "        \"\"\"Create searchable PDF from scanned document\"\"\"\n",
        "        from fpdf import FPDF\n",
        "\n",
        "        # Create PDF\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "\n",
        "        # Add image\n",
        "        image_path = 'temp_image.png'\n",
        "        cv2.imwrite(image_path, image)\n",
        "        pdf.image(image_path, x=10, y=10, w=190)\n",
        "\n",
        "        # Add invisible text layer\n",
        "        pdf.set_font('Arial', '', 12)\n",
        "        pdf.set_text_color(0, 0, 0)\n",
        "        pdf.set_xy(10, 10)\n",
        "\n",
        "        # Split text into lines and add to PDF\n",
        "        lines = text.split('\\n')\n",
        "        for line in lines:\n",
        "            pdf.cell(0, 5, line, ln=True)\n",
        "\n",
        "        return pdf\n",
        "\n",
        "    def extract_form_fields(self, image, text):\n",
        "        \"\"\"Extract structured data from form documents\"\"\"\n",
        "        # Define common field patterns\n",
        "        field_patterns = {\n",
        "            'name': r'Name:?\\s*([A-Za-z\\s]+)',\n",
        "            'date': r'Date:?\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})',\n",
        "            'email': r'Email:?\\s*([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})',\n",
        "            'phone': r'Phone:?\\s*(\\d{3}[-.]?\\d{3}[-.]?\\d{4})',\n",
        "            'address': r'Address:?\\s*([A-Za-z0-9\\s,]+)'\n",
        "        }\n",
        "\n",
        "        # Extract fields using regex\n",
        "        fields = {}\n",
        "        for field_name, pattern in field_patterns.items():\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                fields[field_name] = match.group(1).strip()\n",
        "\n",
        "        return fields\n",
        "\n",
        "    def detect_tables(self, image):\n",
        "        \"\"\"Detect and extract tables from document\"\"\"\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Detect horizontal and vertical lines\n",
        "        horizontal = np.copy(gray)\n",
        "        vertical = np.copy(gray)\n",
        "\n",
        "        # Specify size on horizontal and vertical lines\n",
        "        cols = horizontal.shape[1]\n",
        "        horizontal_size = cols // 30\n",
        "        rows = vertical.shape[0]\n",
        "        vertical_size = rows // 30\n",
        "\n",
        "        # Create structure elements\n",
        "        horizontalStructure = cv2.getStructuringElement(\n",
        "            cv2.MORPH_RECT, (horizontal_size, 1)\n",
        "        )\n",
        "        verticalStructure = cv2.getStructuringElement(\n",
        "            cv2.MORPH_RECT, (1, vertical_size)\n",
        "        )\n",
        "\n",
        "        # Apply morphology operations\n",
        "        horizontal = cv2.erode(horizontal, horizontalStructure)\n",
        "        horizontal = cv2.dilate(horizontal, horizontalStructure)\n",
        "\n",
        "        vertical = cv2.erode(vertical, verticalStructure)\n",
        "        vertical = cv2.dilate(vertical, verticalStructure)\n",
        "\n",
        "        # Combine horizontal and vertical lines\n",
        "        table_mask = cv2.add(horizontal, vertical)\n",
        "\n",
        "        # Find contours of table cells\n",
        "        contours, _ = cv2.findContours(\n",
        "            table_mask,\n",
        "            cv2.RETR_TREE,\n",
        "            cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        # Extract cell contents\n",
        "        cells = []\n",
        "        for contour in contours:\n",
        "            x, y, w, h = cv2.boundingRect(contour)\n",
        "            cell_image = gray[y:y+h, x:x+w]\n",
        "            cell_text = pytesseract.image_to_string(\n",
        "                cell_image,\n",
        "                config=self.scanner.tesseract_config\n",
        "            )\n",
        "            cells.append({\n",
        "                'position': (x, y, w, h),\n",
        "                'text': cell_text.strip()\n",
        "            })\n",
        "\n",
        "        return cells\n",
        "\n",
        "    def enhance_image_quality(self, image):\n",
        "        \"\"\"Advanced image enhancement techniques\"\"\"\n",
        "        # Apply denoising\n",
        "        denoised = cv2.fastNlMeansDenoising(image)\n",
        "\n",
        "        # Apply CLAHE for contrast enhancement\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        enhanced = clahe.apply(denoised)\n",
        "\n",
        "        # Apply sharpening\n",
        "        kernel = np.array([[-1,-1,-1],\n",
        "                         [-1, 9,-1],\n",
        "                         [-1,-1,-1]])\n",
        "        sharpened = cv2.filter2D(enhanced, -1, kernel)\n",
        "\n",
        "        return sharpened\n",
        "\n",
        "    def validate_document(self, text, doc_type):\n",
        "        \"\"\"Validate document completeness and quality\"\"\"\n",
        "        validation_results = {\n",
        "            'is_complete': True,\n",
        "            'issues': [],\n",
        "            'confidence_score': 0.0\n",
        "        }\n",
        "\n",
        "        # Check text extraction quality\n",
        "        if len(text.strip()) < 50:\n",
        "            validation_results['is_complete'] = False\n",
        "            validation_results['issues'].append(\n",
        "                \"Low text content - possible extraction failure\"\n",
        "            )\n",
        "\n",
        "        # Check for required fields based on document type\n",
        "        if doc_type == 'Invoice':\n",
        "            required_patterns = {\n",
        "                'invoice_number': r'Invoice\\s*#?\\s*([A-Za-z0-9-]+)',\n",
        "                'date': r'Date:?\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})',\n",
        "                'amount': r'Total:?\\s*\\$?\\s*(\\d+\\.?\\d*)',\n",
        "            }\n",
        "\n",
        "            for field, pattern in required_patterns.items():\n",
        "                if not re.search(pattern, text):\n",
        "                    validation_results['is_complete'] = False\n",
        "                    validation_results['issues'].append(\n",
        "                        f\"Missing required field: {field}\"\n",
        "                    )\n",
        "\n",
        "        # Calculate confidence score\n",
        "        words = text.split()\n",
        "        valid_words = sum(\n",
        "            1 for word in words\n",
        "            if len(word) > 2 and word.isalnum()\n",
        "        )\n",
        "        validation_results['confidence_score'] = valid_words / len(words)\n",
        "\n",
        "        return validation_results"
      ],
      "metadata": {
        "id": "Gb1Cg-Eos4Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Create scanner instance\n",
        "    scanner = DocumentScanner()\n",
        "    processor = DocumentProcessor(scanner)\n",
        "\n",
        "    # Example batch processing\n",
        "    image_paths = ['doc1.jpg', 'doc2.jpg', 'doc3.jpg']\n",
        "    results = processor.batch_process(image_paths)\n",
        "\n",
        "    # Process and analyze results\n",
        "    for result in results:\n",
        "        # Validate document\n",
        "        validation = processor.validate_document(\n",
        "            result['text'],\n",
        "            result['document_type']\n",
        "        )\n",
        "\n",
        "        if validation['is_complete']:\n",
        "            # Generate searchable PDF\n",
        "            pdf = processor.generate_searchable_pdf(\n",
        "                result['enhanced'],\n",
        "                result['text']\n",
        "            )\n",
        "\n",
        "            # Extract form fields if applicable\n",
        "            if result['document_type'] in ['Form', 'Invoice']:\n",
        "                fields = processor.extract_form_fields(\n",
        "                    result['enhanced'],\n",
        "                    result['text']\n",
        "                )\n",
        "\n",
        "            # Detect and extract tables\n",
        "            tables = processor.detect_tables(result['enhanced'])\n",
        "        else:\n",
        "            print(f\"Document validation failed: {validation['issues']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "CDT5dPsDs4Qn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}